{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.add_jars('/app/postgresql-42.1.4.jar')\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Stocks:ETL\")\n",
    "    .config(\"spark.driver.memory\", \"512m\")\n",
    "    .config(\"spark.driver.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"512m\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_dir = '/dataset/stocks-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# UDF\n",
    "from pyspark.sql.types import StringType\n",
    "#\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(stocks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- OpenInt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.count()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+------+-------+\n",
      "|               Date|  Open|  High|   Low| Close|Volume|OpenInt|\n",
      "+-------------------+------+------+------+------+------+-------+\n",
      "|1962-01-02 00:00:00| 6.413| 6.413|6.3378|6.3378|467056|      0|\n",
      "|1962-01-03 00:00:00|6.3378|6.3963|6.3378|6.3963|350294|      0|\n",
      "|1962-01-04 00:00:00|6.3963|6.3963|6.3295|6.3295|314365|      0|\n",
      "|1962-01-05 00:00:00|6.3211|6.3211|6.1958|6.2041|440112|      0|\n",
      "|1962-01-08 00:00:00|6.2041|6.2041|6.0373| 6.087|655676|      0|\n",
      "|1962-01-09 00:00:00|6.1208|6.2376|6.1208|6.1621|592806|      0|\n",
      "|1962-01-10 00:00:00|6.1707|6.2041|6.1707|6.1707|359274|      0|\n",
      "|1962-01-11 00:00:00|6.1875|6.2376|6.1875|6.2376|386220|      0|\n",
      "|1962-01-12 00:00:00|6.2543|6.2962|6.2543|6.2543|529933|      0|\n",
      "|1962-01-15 00:00:00|6.2708|6.2962|6.2708|6.2792|305383|      0|\n",
      "|1962-01-16 00:00:00|6.2708|6.2708|6.2128|6.2128|305383|      0|\n",
      "|1962-01-17 00:00:00|6.1875|6.1875|6.0956|6.1125|502984|      0|\n",
      "|1962-01-18 00:00:00|6.1291|6.1875|6.1291|6.1291|449093|      0|\n",
      "|1962-01-19 00:00:00|6.1291|6.1457|6.0624|6.1374|485021|      0|\n",
      "|1962-01-22 00:00:00|6.1374|6.1958|6.1208|6.1208|332329|      0|\n",
      "|1962-01-23 00:00:00|6.1208|6.1291|6.0538|6.0624|449093|      0|\n",
      "|1962-01-24 00:00:00|6.0624|6.0956|6.0287|6.0956|494001|      0|\n",
      "|1962-01-25 00:00:00|6.0956|6.1457|6.0208|6.0287|386220|      0|\n",
      "|1962-01-26 00:00:00|6.0287|6.0538|5.9951|5.9951|296401|      0|\n",
      "|1962-01-29 00:00:00|5.9951|6.0373|5.8952|5.8952|700585|      0|\n",
      "+-------------------+------+------+------+------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('filename', F.input_file_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+------+-------+---------------------------------------+\n",
      "|Date               |Open  |High  |Low   |Close |Volume|OpenInt|filename                               |\n",
      "+-------------------+------+------+------+------+------+-------+---------------------------------------+\n",
      "|1962-01-02 00:00:00|6.413 |6.413 |6.3378|6.3378|467056|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-03 00:00:00|6.3378|6.3963|6.3378|6.3963|350294|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-04 00:00:00|6.3963|6.3963|6.3295|6.3295|314365|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-05 00:00:00|6.3211|6.3211|6.1958|6.2041|440112|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-08 00:00:00|6.2041|6.2041|6.0373|6.087 |655676|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-09 00:00:00|6.1208|6.2376|6.1208|6.1621|592806|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-10 00:00:00|6.1707|6.2041|6.1707|6.1707|359274|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-11 00:00:00|6.1875|6.2376|6.1875|6.2376|386220|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-12 00:00:00|6.2543|6.2962|6.2543|6.2543|529933|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-15 00:00:00|6.2708|6.2962|6.2708|6.2792|305383|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-16 00:00:00|6.2708|6.2708|6.2128|6.2128|305383|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-17 00:00:00|6.1875|6.1875|6.0956|6.1125|502984|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-18 00:00:00|6.1291|6.1875|6.1291|6.1291|449093|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-19 00:00:00|6.1291|6.1457|6.0624|6.1374|485021|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-22 00:00:00|6.1374|6.1958|6.1208|6.1208|332329|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-23 00:00:00|6.1208|6.1291|6.0538|6.0624|449093|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-24 00:00:00|6.0624|6.0956|6.0287|6.0956|494001|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-25 00:00:00|6.0956|6.1457|6.0208|6.0287|386220|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-26 00:00:00|6.0287|6.0538|5.9951|5.9951|296401|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-29 00:00:00|5.9951|6.0373|5.8952|5.8952|700585|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "+-------------------+------+------+------+------+------+-------+---------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup = spark.read.csv('/dataset/yahoo-symbols-201709.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------+--------------------+-------+\n",
      "|   _c0|                 _c1|     _c2|                 _c3|    _c4|\n",
      "+------+--------------------+--------+--------------------+-------+\n",
      "|Ticker|                Name|Exchange|       Category Name|Country|\n",
      "|  OEDV|Osage Exploration...|     PNK|                null|    USA|\n",
      "|  AAPL|          Apple Inc.|     NMS|Electronic Equipment|    USA|\n",
      "|   BAC|Bank of America C...|     NYQ|  Money Center Banks|    USA|\n",
      "|  AMZN|    Amazon.com, Inc.|     NMS|Catalog & Mail Or...|    USA|\n",
      "|     T|           AT&T Inc.|     NYQ|Telecom Services ...|    USA|\n",
      "|  GOOG|       Alphabet Inc.|     NMS|Internet Informat...|    USA|\n",
      "|    MO|  Altria Group, Inc.|     NYQ|          Cigarettes|    USA|\n",
      "|   DAL|Delta Air Lines, ...|     NYQ|      Major Airlines|    USA|\n",
      "|    AA|   Alcoa Corporation|     NYQ|            Aluminum|    USA|\n",
      "|   AXP|American Express ...|     NYQ|     Credit Services|    USA|\n",
      "|    DD|E. I. du Pont de ...|     NYQ|Agricultural Chem...|    USA|\n",
      "|  BABA|Alibaba Group Hol...|     NYQ|Specialty Retail,...|    USA|\n",
      "|   ABT| Abbott Laboratories|     NYQ|Medical Appliance...|    USA|\n",
      "|    UA|  Under Armour, Inc.|     NYQ|Textile - Apparel...|    USA|\n",
      "|  AMAT|Applied Materials...|     NMS|Semiconductor Equ...|    USA|\n",
      "|  AMGN|          Amgen Inc.|     NMS|       Biotechnology|    USA|\n",
      "|   AAL|American Airlines...|     NMS|      Major Airlines|    USA|\n",
      "|   AIG|American Internat...|     NYQ|Property & Casual...|    USA|\n",
      "|   ALL|The Allstate Corp...|     NYQ|Property & Casual...|    USA|\n",
      "+------+--------------------+--------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lookup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_symbol_from(filename):\n",
    "    return filename.split('/')[-1].split('.')[0].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IBM'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filename = 'file:///dataset/stocks-small/ibm.us.txt' # => IBM\n",
    "extract_symbol_from('file:///dataset/stocks-small/ibm.us.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_symbol = F.udf(lambda filename: extract_symbol_from(filename), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_folder = stocks_dir\n",
    "df = spark.read \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"inferSchema\", True) \\\n",
    "        .csv(stocks_folder) \\\n",
    "        .withColumn(\"name\", extract_symbol(F.input_file_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+------+-------+----+\n",
      "|               Date|  Open|  High|   Low| Close|Volume|OpenInt|name|\n",
      "+-------------------+------+------+------+------+------+-------+----+\n",
      "|1962-01-02 00:00:00| 6.413| 6.413|6.3378|6.3378|467056|      0| IBM|\n",
      "|1962-01-03 00:00:00|6.3378|6.3963|6.3378|6.3963|350294|      0| IBM|\n",
      "|1962-01-04 00:00:00|6.3963|6.3963|6.3295|6.3295|314365|      0| IBM|\n",
      "|1962-01-05 00:00:00|6.3211|6.3211|6.1958|6.2041|440112|      0| IBM|\n",
      "|1962-01-08 00:00:00|6.2041|6.2041|6.0373| 6.087|655676|      0| IBM|\n",
      "+-------------------+------+------+------+------+------+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"inferSchema\", True) \\\n",
    "        .csv(stocks_folder) \\\n",
    "        .withColumn(\"name\", extract_symbol(F.input_file_name())) \\\n",
    "        .withColumnRenamed(\"Date\", \"dateTime\") \\\n",
    "        .withColumnRenamed(\"Open\", \"open\") \\\n",
    "        .withColumnRenamed(\"High\", \"high\") \\\n",
    "        .withColumnRenamed(\"Low\", \"low\") \\\n",
    "        .withColumnRenamed(\"Close\", \"close\") \\\n",
    "        .drop(\"Volume\", \"OpenInt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+----+\n",
      "|           dateTime|  open|  high|   low| close|name|\n",
      "+-------------------+------+------+------+------+----+\n",
      "|1962-01-02 00:00:00| 6.413| 6.413|6.3378|6.3378| IBM|\n",
      "|1962-01-03 00:00:00|6.3378|6.3963|6.3378|6.3963| IBM|\n",
      "|1962-01-04 00:00:00|6.3963|6.3963|6.3295|6.3295| IBM|\n",
      "|1962-01-05 00:00:00|6.3211|6.3211|6.1958|6.2041| IBM|\n",
      "|1962-01-08 00:00:00|6.2041|6.2041|6.0373| 6.087| IBM|\n",
      "+-------------------+------+------+------+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stocks.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_file = '/dataset/yahoo-symbols-201709.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_lookup = spark.read. \\\n",
    "        option(\"header\", True). \\\n",
    "        option(\"inferSchema\", True). \\\n",
    "        csv(lookup_file). \\\n",
    "        select(\"Ticker\", \"Category Name\"). \\\n",
    "        withColumnRenamed(\"Ticker\", \"symbol\"). \\\n",
    "        withColumnRenamed(\"Category Name\", \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+----+\n",
      "|           dateTime|  open|  high|   low| close|name|\n",
      "+-------------------+------+------+------+------+----+\n",
      "|1962-01-02 00:00:00| 6.413| 6.413|6.3378|6.3378| IBM|\n",
      "|1962-01-03 00:00:00|6.3378|6.3963|6.3378|6.3963| IBM|\n",
      "|1962-01-04 00:00:00|6.3963|6.3963|6.3295|6.3295| IBM|\n",
      "+-------------------+------+------+------+------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+--------------------+\n",
      "|symbol|            category|\n",
      "+------+--------------------+\n",
      "|  OEDV|                null|\n",
      "|  AAPL|Electronic Equipment|\n",
      "|   BAC|  Money Center Banks|\n",
      "+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stocks.show(3)\n",
    "symbols_lookup.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = df_stocks \\\n",
    "    .withColumnRenamed('dateTime', \"full_date\") \\\n",
    "    .filter(\"full_date >= \\\"2017-09-01\\\"\") \\\n",
    "    .withColumn(\"year\", F.year(\"full_date\")) \\\n",
    "    .withColumn(\"month\", F.month(\"full_date\")) \\\n",
    "    .withColumn(\"day\", F.dayofmonth(\"full_date\")) \\\n",
    "    .withColumnRenamed(\"name\", \"symbol\") \\\n",
    "    .join(symbols_lookup, [\"symbol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------+------+------+------+----+-----+---+--------------------+\n",
      "|symbol|          full_date|  open|  high|   low| close|year|month|day|            category|\n",
      "+------+-------------------+------+------+------+------+----+-----+---+--------------------+\n",
      "|   IBM|2017-09-01 00:00:00|141.57|143.07|141.57|142.65|2017|    9|  1|Information Techn...|\n",
      "|   IBM|2017-09-05 00:00:00|142.08|142.93|141.29|141.62|2017|    9|  5|Information Techn...|\n",
      "|   IBM|2017-09-06 00:00:00|142.46|143.04|142.08| 142.4|2017|    9|  6|Information Techn...|\n",
      "+------+-------------------+------+------+------+------+----+-----+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "window20 = (Window.partitionBy(F.col('symbol')).orderBy(F.col(\"full_date\")).rowsBetween(-20, 0))\n",
    "window50 = (Window.partitionBy(F.col('symbol')).orderBy(F.col(\"full_date\")).rowsBetween(-50, 0))\n",
    "window100 = (Window.partitionBy(F.col('symbol')).orderBy(F.col(\"full_date\")).rowsBetween(-100, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_moving_avg_df = joined_df \\\n",
    "    .withColumn(\"ma20\", F.avg(\"close\").over(window20)) \\\n",
    "    .withColumn(\"ma50\", F.avg(\"close\").over(window50)) \\\n",
    "    .withColumn(\"ma100\", F.avg(\"close\").over(window100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+\n",
      "|symbol| close|              ma20|\n",
      "+------+------+------------------+\n",
      "|  AAPL|163.46|            163.46|\n",
      "|  AAPL| 161.5|162.48000000000002|\n",
      "|  AAPL|161.33| 162.0966666666667|\n",
      "|  AAPL|160.68|          161.7425|\n",
      "|  AAPL|158.06|           161.006|\n",
      "|  AAPL|160.92|160.99166666666665|\n",
      "|  AAPL|160.28|160.89000000000001|\n",
      "|  AAPL|159.08|         160.66375|\n",
      "|  AAPL|157.71|160.33555555555554|\n",
      "|  AAPL|159.31|           160.233|\n",
      "|  AAPL| 158.1| 160.0390909090909|\n",
      "|  AAPL|158.16|          159.8825|\n",
      "|  AAPL|155.51|159.54615384615383|\n",
      "|  AAPL|152.84|159.06714285714287|\n",
      "|  AAPL|151.35|158.55266666666665|\n",
      "|  AAPL|150.01|         158.01875|\n",
      "|  AAPL|152.59| 157.6994117647059|\n",
      "|  AAPL|153.68|157.47611111111112|\n",
      "|  AAPL|152.73| 157.2263157894737|\n",
      "|  AAPL|153.57|157.04350000000002|\n",
      "|  AAPL|153.26|156.86333333333334|\n",
      "|  AAPL|153.92| 156.4090476190476|\n",
      "|  AAPL|152.93|156.00095238095238|\n",
      "|  AAPL|154.83|155.69142857142856|\n",
      "|  AAPL|154.74|155.40857142857143|\n",
      "+------+------+------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Moving Average\n",
    "stocks_moving_avg_df.select('symbol', 'close', 'ma20').show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/dataset/output.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_moving_avg_df \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "    .parquet(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "950"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parquet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.createOrReplaceTempView(\"stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[symbol#559], functions=[max(close#564)])\n",
      "+- Exchange hashpartitioning(symbol#559, 2)\n",
      "   +- *(1) HashAggregate(keys=[symbol#559], functions=[partial_max(close#564)])\n",
      "      +- *(1) Project [symbol#559, close#564]\n",
      "         +- *(1) Filter ((isnotnull(full_date#560) && (cast(full_date#560 as string) >= 2017-09-01)) && (cast(full_date#560 as string) < 2017-10-01))\n",
      "            +- *(1) FileScan parquet [symbol#559,full_date#560,close#564,year#569,month#570,day#571] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/dataset/output.parquet], PartitionCount: 50, PartitionFilters: [], PushedFilters: [IsNotNull(full_date)], ReadSchema: struct<symbol:string,full_date:timestamp,close:double>\n"
     ]
    }
   ],
   "source": [
    "badHighestClosingPrice = spark.sql(\"SELECT symbol, MAX(close) AS price FROM stocks WHERE full_date >= '2017-09-01' AND full_date < '2017-10-01' GROUP BY symbol\")\n",
    "badHighestClosingPrice.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[symbol#559], functions=[max(close#564)])\n",
      "+- Exchange hashpartitioning(symbol#559, 2)\n",
      "   +- *(1) HashAggregate(keys=[symbol#559], functions=[partial_max(close#564)])\n",
      "      +- *(1) Project [symbol#559, close#564]\n",
      "         +- *(1) FileScan parquet [symbol#559,close#564,year#569,month#570,day#571] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/dataset/output.parquet], PartitionCount: 20, PartitionFilters: [isnotnull(year#569), isnotnull(month#570), (year#569 = 2017), (month#570 = 9)], PushedFilters: [], ReadSchema: struct<symbol:string,close:double>\n"
     ]
    }
   ],
   "source": [
    "highestClosingPrice = spark.sql(\"SELECT symbol, MAX(close) AS price FROM stocks WHERE year=2017 AND month=9 GROUP BY symbol\")\n",
    "highestClosingPrice.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to Postgres\n",
    "stocks_moving_avg_df \\\n",
    "    .drop(\"year\", \"month\", \"day\") \\\n",
    "    .write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres/workshop\") \\\n",
    "    .option(\"dbtable\", \"workshop.stocks\") \\\n",
    "    .option(\"user\", \"workshop\") \\\n",
    "    .option(\"password\", \"w0rkzh0p\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode('append') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
